{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7829b8b-7114-4f2e-963e-1381c50a0dda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Sales Prediction using Lakehouse Data\n",
    "\n",
    "## Step 1: Create a Feature Dataset for Machine Learning\n",
    "\n",
    "In this step, we create a machine-learning–ready feature dataset by joining\n",
    "transactional sales data (`fact_sales`) with product attributes (`dim_products`).\n",
    "\n",
    "Why this step is important:\n",
    "- Raw fact tables are not always directly suitable for ML\n",
    "- Joining dimension data enriches features and improves model quality\n",
    "- This mirrors real-world feature engineering in production systems\n",
    "\n",
    "The resulting dataset will be used in subsequent steps for training a sales prediction model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f498b5e9-061c-4d42-9822-095d2b683bdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98566808-2c6a-4102-aead-f01882274e59",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, size, slice, concat_ws\n",
    "\n",
    "# Load Gold tables\n",
    "fact_df = spark.table(\"bike_data.gold.fact_sales\")\n",
    "dim_products_df = spark.table(\"bike_data.gold.dim_products\")\n",
    "\n",
    "# Transform product_key in dim_products to match fact_sales format\n",
    "dim_products_transformed = dim_products_df.withColumn(\n",
    "    \"product_key_fact\",\n",
    "    concat_ws(\n",
    "        \"-\",\n",
    "        slice(\n",
    "            split(dim_products_df.product_key, \"-\"),\n",
    "            size(split(dim_products_df.product_key, \"-\")) - 2,\n",
    "\n",
    "            3\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Join fact and dimension tables\n",
    "feature_df = (\n",
    "    fact_df.alias(\"f\")\n",
    "    .join(\n",
    "        dim_products_transformed.alias(\"p\"),\n",
    "        col(\"f.product_key\") == col(\"p.product_key_fact\"),\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Basic validation\n",
    "feature_df.printSchema()\n",
    "feature_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c8fcb89-f4f1-45de-b24c-f3bfc8caf567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Target Variable and Select Features for ML\n",
    "\n",
    "In this step, we prepare the dataset for machine learning by:\n",
    "- Identifying the target variable (`sales`)\n",
    "- Selecting relevant numerical and categorical features\n",
    "- Removing columns that are identifiers or not suitable for model training\n",
    "\n",
    "The goal is to keep the feature set simple, meaningful, and aligned with\n",
    "real-world availability at prediction time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d910337-efe5-435c-a56a-fd07dacc3c5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Select target and feature columns\n",
    "\n",
    "ml_df = feature_df.select(\n",
    "    \"sales\",            # target variable\n",
    "    \"quantity\",\n",
    "    \"price\",\n",
    "    \"product_cost\",\n",
    "    \"category\",\n",
    "    \"subcategory\"\n",
    ")\n",
    "\n",
    "# Basic validation\n",
    "ml_df.printSchema()\n",
    "ml_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a086e94-0ae5-4329-aebf-7dc31c04c3cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Encode Categorical Features\n",
    "\n",
    "Machine learning algorithms require numerical inputs.\n",
    "In this step, categorical features are converted into numerical form using\n",
    "StringIndexer.\n",
    "\n",
    "Why StringIndexer:\n",
    "- Assigns unique numeric indices to categories\n",
    "- Handles unseen or invalid values safely\n",
    "- Scales well for large datasets in Spark\n",
    "\n",
    "This step prepares categorical columns for feature vector assembly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49e3523f-7f7b-4e03-a6d0-c7bb1dac12b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Index categorical columns\n",
    "category_indexer = StringIndexer(\n",
    "    inputCol=\"category\",\n",
    "    outputCol=\"category_idx\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "subcategory_indexer = StringIndexer(\n",
    "    inputCol=\"subcategory\",\n",
    "    outputCol=\"subcategory_idx\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# Fit and transform the dataset\n",
    "ml_df_indexed = category_indexer.fit(ml_df).transform(ml_df)\n",
    "ml_df_indexed = subcategory_indexer.fit(ml_df_indexed).transform(ml_df_indexed)\n",
    "\n",
    "# Validate output\n",
    "ml_df_indexed.printSchema()\n",
    "ml_df_indexed.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0903002-4ae7-4125-9660-712cc472dc08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##  Assemble Feature Vector\n",
    "\n",
    "In this step, all selected numerical and encoded categorical features\n",
    "are combined into a single feature vector.\n",
    "\n",
    "Why this step is required:\n",
    "- Spark ML models expect a single `features` column\n",
    "- VectorAssembler consolidates multiple feature columns efficiently\n",
    "- This keeps the pipeline clean and scalable\n",
    "\n",
    "Only relevant, model-ready columns are included in the feature vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dbbd7ec-2ce3-4ed0-82a6-a4f63742f11c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Assemble numerical and indexed categorical features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"quantity\",\n",
    "        \"price\",\n",
    "        \"product_cost\",\n",
    "        \"category_idx\",\n",
    "        \"subcategory_idx\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "ml_df_final = assembler.transform(ml_df_indexed)\n",
    "\n",
    "# Validate assembled features\n",
    "ml_df_final.select(\"features\", \"sales\").display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ade0f330-e12c-4417-b8c3-fcee9aaba521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Train–Test Split\n",
    "\n",
    "To evaluate the model fairly, the dataset is split into training and test sets.\n",
    "The training set is used to learn patterns, while the test set is used to\n",
    "evaluate performance on unseen data.\n",
    "\n",
    "A fixed random seed is used to ensure reproducibility of results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b87ef2c0-4e06-49d9-b598-6c40b28afacb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: Split data into training and testing sets\n",
    "\n",
    "train_df, test_df = ml_df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Basic validation\n",
    "print(f\"Training records: {train_df.count()}\")\n",
    "print(f\"Testing records: {test_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9db5fa70-11c3-4bad-91b5-dec917df9679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: Model Selection and Training\n",
    "\n",
    "In this step, a regression model is trained to predict `sales` based on the\n",
    "assembled feature vector.\n",
    "\n",
    "A **Linear Regression** model is chosen as a baseline because:\n",
    "- The target variable (`sales`) is continuous\n",
    "- Linear Regression is simple, fast, and interpretable\n",
    "- It provides a strong baseline before exploring complex models\n",
    "\n",
    "The model is trained using the training dataset created in the previous step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63d0659f-08b2-454f-8be0-f1be2540ab3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Initialize Linear Regression model\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"sales\"\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "print(\"Linear Regression model training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07b02e55-0402-4d36-ad1b-8d8733b3ce2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate Predictions\n",
    "\n",
    "In this step, the trained regression model is applied to the test dataset\n",
    "to generate sales predictions.\n",
    "\n",
    "These predictions will be used in the next steps to:\n",
    "- Evaluate model performance\n",
    "- Understand how close predicted values are to actual sales\n",
    "- Validate whether the model learns meaningful patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a8bf1c8-3562-463b-b43d-23aa41f6570f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 7: Generate predictions on test data\n",
    "\n",
    "predictions_df = lr_model.transform(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "645220b5-87f6-42fe-b176-a299c1b8cb79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_predictions_df = predictions_df.select(\n",
    "    \"quantity\",\n",
    "    \"price\",\n",
    "    \"product_cost\",\n",
    "    \"category\",\n",
    "    \"subcategory\",\n",
    "    \"sales\",\n",
    "    \"prediction\"\n",
    ")\n",
    "\n",
    "final_predictions_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f278139-d8dc-4600-ac04-8332c2b860d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 8: Model Evaluation\n",
    "\n",
    "In this step, we evaluate the performance of the regression model using\n",
    "standard regression metrics.\n",
    "\n",
    "Evaluation helps answer:\n",
    "- How close are predicted sales values to actual sales?\n",
    "- How well does the model explain variance in the data?\n",
    "\n",
    "Two metrics are used:\n",
    "- RMSE (Root Mean Squared Error): Measures average prediction error magnitude\n",
    "- R² (Coefficient of Determination): Indicates how much variance is explained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a9b18bf-c5e0-40d8-a0b0-7af511a7e97c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# RMSE evaluation\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"sales\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "rmse = rmse_evaluator.evaluate(predictions_df)\n",
    "\n",
    "# R2 evaluation\n",
    "r2_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"sales\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "r2 = r2_evaluator.evaluate(predictions_df)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R2 Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53c2d6a3-8944-449a-898f-d72ac5903f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 9: Experiment Tracking with MLflow\n",
    "\n",
    "In this step, the trained model and evaluation metrics are logged using MLflow.\n",
    "\n",
    "Why MLflow is used:\n",
    "- Tracks experiments and metrics\n",
    "- Stores trained model artifacts\n",
    "- Enables reproducibility and comparison of runs\n",
    "- Demonstrates end-to-end ML lifecycle management in Databricks\n",
    "\n",
    "This completes the machine learning workflow for sales prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fef62edf-fae1-4277-b632-c1bce4e0f28d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set MLflow temp directory to Unity Catalog volume\n",
    "os.environ[\"MLFLOW_DFS_TMP\"] = \"/Volumes/bike_data/ml/mlflow_tmp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bbd315b-acea-43de-95ea-ca39a6712522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS bike_data.ml;\n",
    "CREATE VOLUME IF NOT EXISTS bike_data.ml.mlflow_tmp;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b467508e-4d71-49e9-aaad-0ba9c30dbc37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Set MLflow experiment\n",
    "mlflow.set_experiment(\"/Shared/sales_prediction_experiment\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    # Log evaluation metrics\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "    \n",
    "    # Log trained model\n",
    "    mlflow.spark.log_model(lr_model, \"linear_regression_model\")\n",
    "    \n",
    "    print(\"Model and metrics logged to MLflow successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a017b406-8b98-4f98-9450-ec4121a7fbd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Persisting Predictions Back to Delta Tables\n",
    "\n",
    "To complete the Database ↔ AI workflow, model predictions are written back\n",
    "to a Gold-layer Delta table. This enables downstream analytics, reporting,\n",
    "and business consumption of AI outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e7519f8-9a73-44ed-b0fe-f81aafda9536",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "final_predictions_df = predictions_df.select(\n",
    "    \"quantity\",\n",
    "    \"price\",\n",
    "    \"product_cost\",\n",
    "    \"category\",\n",
    "    \"subcategory\",\n",
    "    \"sales\",\n",
    "    \"prediction\"\n",
    ").withColumn(\n",
    "    \"prediction_ts\",\n",
    "    current_timestamp()\n",
    ")\n",
    "\n",
    "final_predictions_df.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"bike_data.gold.sales_predictions\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c28e3290-3351-47fe-be9e-b4921622c7a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_predictions_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21ae6c5d-320d-4245-b96a-9523e24e91d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we implemented an end-to-end machine learning workflow on top of a\n",
    "modern lakehouse architecture using Databricks.\n",
    "\n",
    "Starting from analytics-ready Gold tables, we created a feature dataset by joining\n",
    "transactional sales data with product dimension attributes. This ensured that the\n",
    "model was trained on clean, enriched, and business-relevant data.\n",
    "\n",
    "A baseline Linear Regression model was trained to predict sales values, demonstrating\n",
    "how machine learning can be applied to support revenue forecasting and planning use cases.\n",
    "Model performance was evaluated using RMSE and R² metrics, and experiments were tracked\n",
    "using MLflow to ensure reproducibility and transparency.\n",
    "\n",
    "This approach highlights the seamless integration between data engineering and AI\n",
    "workloads, where Delta tables serve as both analytics and ML feature sources.\n",
    "The solution is scalable and can be further enhanced with advanced models,\n",
    "additional feature engineering, or real-time prediction use cases.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8756547706457777,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Sales Prediction Workflow Using Lakehouse and MLflow",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
